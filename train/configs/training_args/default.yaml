_target_: transformers.TrainingArguments
output_dir: ${hydra:runtime.output_dir}
eval_strategy: steps
eval_steps: 200
save_strategy: steps
save_steps: 1000
max_grad_norm: 1.0
prediction_loss_only: true
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 32
max_steps: 50000
optim: adamw_torch
learning_rate: 5e-4
weight_decay: 0.0
lr_scheduler_type: inverse_sqrt
warmup_steps: 1000
logging_strategy: steps
logging_steps: 20
logging_first_step: true
load_best_model_at_end: true
metric_for_best_model: eval_loss
bf16: true
bf16_full_eval: true
fp16: false
disable_tqdm: false
dataloader_drop_last: false
fsdp: ''
report_to: none
split_batches: true
eval_on_start: false
save_safetensors: false
